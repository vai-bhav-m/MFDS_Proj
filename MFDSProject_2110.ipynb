{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2f312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mughal Empire was founded by Babur, a Central Asian ruler, who reigned from 1526–1530. He descended from the Turco-Mongol conqueror Timur on his father's side, and from Genghis Khan on his mother's side.\n"
     ]
    }
   ],
   "source": [
    "ideal = \"The Mughal Empire was founded by Babur, a Central Asian ruler, who reigned from 1526–1530. He descended from the Turco-Mongol conqueror Timur on his father's side, and from Genghis Khan on his mother's side.\"\n",
    "print(ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d36140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc7fc5",
   "metadata": {},
   "source": [
    "Using Regex to first make a list of numbers in the best answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a40ac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1526', '1530']\n"
     ]
    }
   ],
   "source": [
    "numbers = re.sub(\"\\D\",\" \",ideal).split()\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf79ad",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16edadff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mughal Empire was founded by Babur, a Central Asian ruler, who reigned from 1526–1530.\n",
      "\n",
      "He descended from the Turco-Mongol conqueror Timur on his father's side, and from Genghis Khan on his mother's side.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(ideal)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f445f1c",
   "metadata": {},
   "source": [
    "Word Tokenizing using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b377986e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Mughal', 'Empire', 'was', 'founded', 'by', 'Babur', 'a', 'Central', 'Asian', 'ruler', 'who', 'reigned', 'from']\n",
      "\n",
      "['He', 'descended', 'from', 'the', 'Turco', 'Mongol', 'conqueror', 'Timur', 'on', 'his', 'father', 's', 'side', 'and', 'from', 'Genghis', 'Khan', 'on', 'his', 'mother', 's', 'side']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for sentence in sentences:\n",
    "    words = re.sub(\"[^a-zA-Z]\",\" \",sentence).split()\n",
    "    print(words)\n",
    "    print()\n",
    "    word_list.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d0eefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Mughal',\n",
       " 'Empire',\n",
       " 'founded',\n",
       " 'Babur',\n",
       " 'Central',\n",
       " 'Asian',\n",
       " 'ruler',\n",
       " 'reigned',\n",
       " 'He',\n",
       " 'descended',\n",
       " 'Turco',\n",
       " 'Mongol',\n",
       " 'conqueror',\n",
       " 'Timur',\n",
       " 'father',\n",
       " 'side',\n",
       " 'Genghis',\n",
       " 'Khan',\n",
       " 'mother',\n",
       " 'side']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = [word for word in word_list if word not in set(stopwords.words('english'))]\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728ec8e",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "We'll lemmatize cuz it's more accurate than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da619ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Mughal', 'Empire', 'found', 'Babur', 'Central', 'Asian', 'ruler', 'reign', 'He', 'descend', 'Turco', 'Mongol', 'conqueror', 'Timur', 'father', 'side', 'Genghis', 'Khan', 'mother', 'side']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in word_list]\n",
    "print(lemm_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d413f357",
   "metadata": {},
   "source": [
    "Segregating proper nouns using **part of speech tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390aa02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mughal', 'Empire', 'Babur', 'Central', 'Asian', 'Turco', 'Mongol', 'Timur', 'Genghis', 'Khan']\n"
     ]
    }
   ],
   "source": [
    "ppn_list = []\n",
    "tagged = nltk.pos_tag(lemm_words)\n",
    "for (word, tag) in tagged:\n",
    "    if tag == 'NNP': # If the word is a proper noun\n",
    "        ppn_list.append(word)\n",
    "print(ppn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5614641",
   "metadata": {},
   "source": [
    "Hence, we have made a list of the following attributes of ideal answer:\n",
    "1. Numbers\n",
    "2. Proper Nouns\n",
    "3. Common nouns and other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d597866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of numbers:  ['1530', '1526']\n",
      "List of Proper Nouns:  ['Asian', 'Turco', 'Empire', 'Timur', 'Babur', 'Khan', 'Genghis', 'Central', 'Mongol', 'Mughal']\n",
      "List of Common Nouns and other words:  ['ruler', 'father', 'found', 'reign', 'side', 'conqueror', 'mother', 'descend']\n"
     ]
    }
   ],
   "source": [
    "ppn_list = list(set(ppn_list))\n",
    "cmn_list = [word.lower() for word in list(set(lemm_words) - set(ppn_list))]\n",
    "cmn_list = [word for word in cmn_list if word not in set(stopwords.words('english'))]\n",
    "num_list = list(set(numbers))\n",
    "\n",
    "print(\"List of numbers: \", num_list)\n",
    "print(\"List of Proper Nouns: \", ppn_list)\n",
    "print(\"List of Common Nouns and other words: \", cmn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a23ed",
   "metadata": {},
   "source": [
    "Defining a single function representing all transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a337bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_attributes_ideal(answer):\n",
    "    num_list = list(set(re.sub(\"\\D\",\" \",answer).split()))\n",
    "    \n",
    "    # Sentence Tokenization\n",
    "    sentences = nltk.sent_tokenize(answer)\n",
    "    # Word Tokenization with Regex\n",
    "    word_list = []\n",
    "    for sentence in sentences:\n",
    "        words = re.sub(\"[^a-zA-Z]\",\" \",answer).split()\n",
    "        words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "        word_list.extend(words)\n",
    "    \n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in word_list]\n",
    "    \n",
    "    # Segregating Proper Nouns\n",
    "    ppn_list = []\n",
    "    tagged = nltk.pos_tag(lemm_words)\n",
    "    for (word, tag) in tagged:\n",
    "        if tag == 'NNP': # If the word is a proper noun\n",
    "            ppn_list.append(word)\n",
    "    ppn_list = list(set(ppn_list))\n",
    "    \n",
    "    # Segregating Common Nouns and other words\n",
    "    cmn_list = [word.lower() for word in list(set(lemm_words) - set(ppn_list))]\n",
    "    cmn_list = [word for word in cmn_list if word not in set(stopwords.words('english'))]\n",
    "\n",
    "    return num_list, ppn_list, cmn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_attributes_attempt(correct_cmnlist, answer):\n",
    "    num_list = list(set(re.sub(\"\\D\",\" \",answer).split()))\n",
    "    \n",
    "    # Sentence Tokenization\n",
    "    sentences = nltk.sent_tokenize(answer)\n",
    "    # Word Tokenization with Regex\n",
    "    word_list = []\n",
    "    for sentence in sentences:\n",
    "        words = re.sub(\"[^a-zA-Z]\",\" \",answer).split()\n",
    "        words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "        word_list.extend(words)\n",
    "    \n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in word_list]\n",
    "    \n",
    "    # Segregating Proper Nouns\n",
    "    ppn_list = []\n",
    "    tagged = nltk.pos_tag(lemm_words)\n",
    "    for (word, tag) in tagged:\n",
    "        if tag == 'NNP': # If the word is a proper noun\n",
    "            ppn_list.append(word)\n",
    "    ppn_list = list(set(ppn_list))\n",
    "    \n",
    "    # Segregating Common Nouns and other words\n",
    "    cmn_list = [word.lower() for word in list(set(lemm_words) - set(ppn_list))]\n",
    "    cmn_list = [word for word in cmn_list if word not in set(stopwords.words('english'))]\n",
    "\n",
    "    # Synonym matching for common nouns\n",
    "    for word in correct_cmnlist:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for i in syn.lemmas():\n",
    "                for k in range(len(cmn_list)):\n",
    "                    if i.name() == cmn_list[k]:\n",
    "                        cmn_list[k] = word\n",
    "\n",
    "    return num_list, ppn_list, cmn_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ea05c",
   "metadata": {},
   "source": [
    "## Vectorizing and grading an Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cfcf9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = \"The Mughal Empire was founded by Babur, an Asian ruler, who reigned till 1530. He descended from Timur on his father's side, and from Genghis Khan on his mother's side.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aece1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightage = [0.5, 0.3, 0.2]  # Weightage split of 5:3:2 between proper_nouns : numbers : common_nouns\n",
    "\n",
    "def vectoriser(ideal, attempt):\n",
    "    word_set = list(set(ideal).union(set(attempt)))\n",
    "\n",
    "    word_count_ideal = {}\n",
    "    word_count_attempt = {}\n",
    "    for word in word_set:\n",
    "        word_count_ideal[word] = 0\n",
    "        word_count_attempt[word] = 0\n",
    "    for word in ideal:\n",
    "        word_count_ideal[word] += 1\n",
    "    for word in attempt:\n",
    "        word_count_attempt[word] += 1\n",
    "\n",
    "    return list(word_count_ideal.values()),list(word_count_attempt.values())\n",
    "\n",
    "def scoreit(ideal_vec, attempt_vec):\n",
    "    sim = np.dot(ideal_vec, attempt_vec)\n",
    "    if sim != 0:\n",
    "        sim = sim / (norm(ideal_vec)*norm(attempt_vec))\n",
    "    return sim\n",
    "\n",
    "ppn_vec_ideal, ppn_vec_attempt = vectoriser(answer_attributes_ideal(ideal)[1], answer_attributes(attempt)[1])\n",
    "ppn_score = scoreit(ppn_vec_ideal, ppn_vec_attempt)\n",
    "\n",
    "num_vec_ideal, num_vec_attempt = vectoriser(answer_attributes_ideal(ideal)[0], answer_attributes(attempt)[0])\n",
    "num_score = scoreit(num_vec_ideal, num_vec_attempt)\n",
    "\n",
    "cmn_vec_ideal, cmn_vec_attempt = vectoriser(answer_attributes_ideal(ideal)[2], answer_attributes_attempt(answer_attributes_ideal(ideal)[2],attempt)[2])\n",
    "cmn_score = scoreit(cmn_vec_ideal, cmn_vec_attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aeb3e6",
   "metadata": {},
   "source": [
    "Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28dff29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score (out of 100) =  80.5462047623002\n"
     ]
    }
   ],
   "source": [
    "Final = weightage[0] * ppn_score + weightage[1] * num_score + weightage[2] * cmn_score \n",
    "print(\"Final Score (out of 100) = \", Final * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f5951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
